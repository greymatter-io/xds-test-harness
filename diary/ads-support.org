#+TITLE: Ads Support
#+DATE: 2021-01-18
#+AUTHOR: Zach Mandeville, Mark Roth

** Introduction
This diary is a work-in-progress to document how we added ADS support to the xDS
test harness. It sets up a background of the problem, our design of the work,
and a work diary of the progress to implement it.

The bottom of the document has open questions for which I'd love feedback. When
a question is answered, it'll be removed from that section and its answer woven
into this diary.

** Background
The xDS transport protocol has [[https://www.envoyproxy.io/docs/envoy/latest/api-docs/xds_protocol#variants-of-the-xds-transport-protocol][four variants]]:
- State of the World, non-aggregated
- SOTW, aggregated
- Incremental(or delta), non-aggregated
- Incremental, aggregated

Each of the variants are handled by [[https://www.envoyproxy.io/docs/envoy/latest/api-docs/xds_protocol#rpc-services-and-methods-for-each-variant][different RPC services/methods]], and an xDS
server can support any or all of them at the same time.

For example, for LDS, the various protocol variants are provided via the
following RPC methods:
- SotW, non aggregated: ListenerDiscoveryService.StreamListeners
- SOTW, Aggregated : AggregatedDiscoveryService.StreamAggregatedResources
- Incremental, non-agg: ListenerDiscoveryService.DeltaListeners
- Incremental, Aggregated: AggregatedDiscoveryService.DeltaAggregatedResources

For  SotW and incremental, the actual communication on the stream will be
essentially the same for both aggregated and non-aggregated, regardless of which
of the above RPC methods you use to create the stream.

For our tests, then, when a test is not variant specific, we should run the same
test for each qualifying variant, by opening a stream with the correct RPC. For
example, with a basic LDS test, our test runner should be able to run it against
both ~ListenerDiscoveryService.StreamListeners~ and
~AggregatedDisocveryService.StreamAggregatedResources~.

A target server may be configured for any subset of combinations from the four
variants, and we should not lock in any required configuration for our tests.
Instead, through something like command line flags, a person can tell the test
runner which variants their implementation supports.
** Design
*** Specifying variants with cli flags
A person should only start the runner once, and then the runner executes the
test suite against the different set variants, by running the right stream
against each of the supported, appropriate RPC methods (as outlined above).

A person can indicate which variants their target supports using the flag
~--variants~. This flag expects a four character stirng made up of 1's and 0's.
1 represents support, 0 non-support. The variants are, right to left, sotw
non-agg, sotw agg, incremental non-agg, incremental agg.

So if a person wanted to run the test suite against a server
running only non-aggregated, but supporting sotw and incremental, they'd run:
: ./runner --variants 1010

If they are an edge case that only supports SoTW, non-aggregated and
incremental, aggregated, then they'd run:
: ./runner --variants 1001
*** Tagging  the tests
For the tests themselves, some may be general enough to work across all
variants, and some may only be appropriate for a particular combination. We can
indicate this using [[https://github.com/ii/xds-test-harness/blob/ads-work/features/subscriptions.feature#L8][tags added to the top of the test scenario]]. The tags are:
- @sotw
- @incremental
- @aggregate
- @separate
*** reading flags and tags in our main
In our main function, we [[https://github.com/ii/xds-test-harness/blob/ads-work/main.go#L101][parse the variant flag]] to build an array of boolean values.
For each true value in the array, we [[https://github.com/ii/xds-test-harness/blob/ads-work/main.go#L152][run the test suite]] for that variant. This means
the suite can run from 1-4 times.

*** Changes to our test Steps

The main flow for our tests is to set up state on the target server using the
adapter, then [[https://github.com/ii/xds-test-harness/blob/ads-work/internal/runner/services.go#L39][initialize a service]] for the duration of the test. This service
includes channels for requests and responses (and caches for both). Each test
step in the scenario can use this service interface to pass along new requests
or update state as nceeded.

We can use the same pattern for ADS, with some modifications:

Originally, the service interface included its typeURL value.
When we needed to make a new request, we'd use the service's assign typeURL.
This doesn't work for ADs, so now [[https://github.com/ii/xds-test-harness/blob/ads-work/internal/runner/steps.go#L102][our new request functions pass the required
typeURL]], which is determined from the test step itself.

Originally, our subscribing step assumed you would be doing one subscription per
scenario, and so built a service interface as part of the subscription step.
This doesn't work for ADS-Only tests where you are subscribing to multiple
services across the same stream, and building a new service interface means
clearing the existing caches. Now, the subscribing function [[https://github.com/ii/xds-test-harness/blob/ads-work/internal/runner/steps.go#L107][checks if we
already]] have a service initialized and, if so, uses its existing inferface. I
think this change makes the function stronger overall, and may help me fix the
unsubscribing issues.

*** Uncertain changes
I added a new step at the end of our ADS-only test, that reads:
[[https://github.com/ii/xds-test-harness/blob/ads-work/features/subscriptions.feature#L192]["And the server never responds more than necessary"]].

This step is functionally identical to our current step [[https://github.com/ii/xds-test-harness/blob/ads-work/internal/runner/steps.go#L257]["And the client ACKS to
which the server does not respond".]]

I duplicated the function due to an issue in the function's logic. It closes
down our response and request channels, so that any remaining messages can come
through and we can look at the entire range of messages sent. Then, we count
the # of responses and # of requests. Since ACK'ing is built into the client's
lifecycle, and it is acking every response it gets from the server, than in this
final step there should be 1 more request than response. If not, the step fails.

This logic requires the step to be run at the end of the test. However, in the
test cases doc, for the ADS-only test there are multiple "and the client sends
an ack to which the server doesn't respond". If I tried to match the test case
verbatim, we'd be closing the ADS stream too early. If I just put the existing
step at the end, it seems as if we are only ACK'ing once.

Now, the ACK'ing is assumed to happen as part of the test, and we just check at
the end that the server hasn't sent more responses than it needed to. I worry
this makes the test steps too opaque, and it may be highlighting that the
existing steps are not as elegant as they could be. I am v. open for feedback or
suggestions on if this needs to change, and how it could be improved.
** Process
*** DONE Implement hook for switching between variants
*** DONE Refactor functions as necessary
*** DONE Write ADS only test
*** TODO Celebrate and Dance
** Questions
- Does the wording of the ADS test work for everyone?
  + does the test run as we'd expect?
- Are there any objections/improvements to the --variants flag?
